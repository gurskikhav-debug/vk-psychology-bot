import os
import json
import requests
from datetime import datetime, timedelta
from deep_translator import GoogleTranslator
import feedparser

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
TOKEN = os.getenv("TOKEN")
ADMIN_ID = os.getenv("ADMIN_ID")

# --- –ö–µ—à ---
CACHE_FILE = "cache/psych_cache.json"

def load_cache():
    if os.path.exists(CACHE_FILE):
        try:
            with open(CACHE_FILE, "r", encoding="utf-8") as f:
                data = json.load(f)
                return set(data) if isinstance(data, list) else set()
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∫–µ—à–∞: {e}")
    return set()

def save_cache(cache_set):
    os.makedirs("cache", exist_ok=True)
    with open(CACHE_FILE, "w", encoding="utf-8") as f:
        json.dump(list(cache_set), f, ensure_ascii=False, indent=2)

# --- –ü–µ—Ä–µ–≤–æ–¥ ---
def translate_text(text):
    try:
        return GoogleTranslator(source='auto', target='ru').translate(text)
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø–µ—Ä–µ–≤–æ–¥–∞: {e}")
        return text

# --- –ò—Å—Ç–æ—á–Ω–∏–∫–∏: 5 —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö, 5 –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö ---
SOURCES = [
    # üá∑üá∫ –†—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–µ
    {"url": "https://habr.com/ru/rss/search/?q=–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è&target_type=posts&order=date", "name": "Habr: –ü—Å–∏—Ö–æ–ª–æ–≥–∏—è", "lang": "ru"},
    {"url": "https://nplus1.ru/rss", "name": "N+1: –ù–∞—É–∫–∞", "lang": "ru"},
    {"url": "https://tjournal.ru/rss", "name": "TJournal: –°–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ", "lang": "ru"},
    {"url": "https://vc.ru/search/rss?query=–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è", "name": "VC.ru: –ü—Å–∏—Ö–æ–ª–æ–≥–∏—è", "lang": "ru"},
    {"url": "https://arzamas.academy/courses?rss", "name": "Arzamas: –ü—Å–∏—Ö–æ–ª–æ–≥–∏—è", "lang": "ru"},

    # üåç –ê–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ
    {"url": "https://www.reddit.com/r/Psychology.rss", "name": "Reddit: Psychology", "lang": "en"},
    {"url": "https://www.psychologytoday.com/us/blog/the-athletes-way/rss2.xml", "name": "Psychology Today", "lang": "en"},
    {"url": "https://rss.sciencedaily.com/mind_brain/psychology.xml", "name": "ScienceDaily: Psychology", "lang": "en"},
    {"url": "https://www.sciencenews.org/category/psychology/feed", "name": "ScienceNews: Psychology", "lang": "en"},
    {"url": "https://theconversation.com/health/rss", "name": "The Conversation: Health", "lang": "en"}
]

# --- –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞ ---
KEYWORDS = [
    '–ø—Å–∏—Ö–æ–ª–æ–≥–∏—è', '—Ç–µ—Ä–∞–ø–∏—è', '–º–æ—Ç–∏–≤–∞—Ü–∏—è', '—Å–∞–º–æ—Ä–∞–∑–≤–∏—Ç–∏–µ', '—Ç—Ä–µ–≤–æ–∂–Ω–æ—Å—Ç—å',
    '–¥–µ–ø—Ä–µ—Å—Å–∏—è', '–æ—Å–æ–∑–Ω–∞–Ω–Ω–æ—Å—Ç—å', '–∫–æ—É—á', '–ø—Å–∏—Ö–æ–ª–æ–≥', 'mental health',
    'therapy', 'mindfulness', 'self-improvement', 'anxiety', 'depression',
    'counseling', 'psychotherapy', 'wellbeing', 'emotional health'
]

# --- –û—Ç–ø—Ä–∞–≤–∫–∞ –≤ Telegram ---
def send_message(chat_id, text, parse_mode=None, disable_preview=False):
    if not chat_id:
        print("‚ùå chat_id –Ω–µ –∑–∞–¥–∞–Ω")
        return
    try:
        url = f"https://api.telegram.org/bot{TOKEN}/sendMessage"
        data = {
            "chat_id": chat_id,
            "text": text,
            "parse_mode": parse_mode,
            "disable_web_page_preview": not disable_preview
        }
        response = requests.post(url, data=data, timeout=10)
        if response.status_code == 200:
            print(f"‚úÖ –°–æ–æ–±—â–µ–Ω–∏–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ –≤ {chat_id}")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏: {response.status_code}, {response.text}")
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ: {e}")

# --- –ü–æ–∏—Å–∫ —Å—Ç–∞—Ç–µ–π –∑–∞ 7 –¥–Ω–µ–π ---
def search_articles():
    articles = []
    from_date = datetime.now() - timedelta(days=7)  # 1 –Ω–µ–¥–µ–ª—è

    for src in SOURCES:
        try:
            feed = feedparser.parse(src["url"])
            if feed.bozo:
                print(f"‚ö†Ô∏è RSS {src['name']} –ø–æ–≤—Ä–µ–∂–¥—ë–Ω")
                continue

            for entry in feed.entries:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–∞—Ç—É
                published = entry.get('published_parsed') or entry.get('updated_parsed')
                if not published:
                    continue
                pub_date = datetime(*published[:6])
                if pub_date < from_date:
                    continue

                title = entry.get('title', '').lower()
                summary = entry.get('summary', '').lower()

                # –§–∏–ª—å—Ç—Ä –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º
                if any(kw.lower() in title or kw.lower() in summary for kw in KEYWORDS):
                    articles.append({
                        'title': entry.get('title', '–ë–µ–∑ –∑–∞–≥–æ–ª–æ–≤–∫–∞'),
                        'url': entry.get('link'),
                        'source': src["name"],
                        'lang': src["lang"],
                        'published': pub_date.isoformat()
                    })
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ RSS {src['name']}: {e}")

    return articles

# --- –û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è ---
def main():
    print("üöÄ –ë–æ—Ç –∑–∞–ø—É—â–µ–Ω (—Ä–µ–∂–∏–º GitHub Actions)")
    try:
        seen_urls = load_cache()
        raw_articles = search_articles()
        print(f"–ü–æ–ª—É—á–µ–Ω–æ —Å—Ç–∞—Ç–µ–π: {len(raw_articles)}")

        # –§–∏–ª—å—Ç—Ä—É–µ–º –¥—É–±–ª–∏
        new_articles = [a for a in raw_articles if a.get('url') not in seen_urls]

        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º 20 –Ω–æ–≤–æ—Å—Ç—è–º–∏
        selected = new_articles[:20]
        print(f"–û—Ç–ø—Ä–∞–≤–ª—è–µ–º: {len(selected)} –Ω–æ–≤–æ—Å—Ç–µ–π")

        if not selected:
            print("–ù–µ—Ç –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.")
            if ADMIN_ID:
                try:
                    admin_id_int = int(ADMIN_ID)
                    send_message(admin_id_int, "üì≠ –ù–æ–≤—ã—Ö —Å—Ç–∞—Ç–µ–π –ø–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ –ø–æ–∫–∞ –Ω–µ—Ç.", disable_preview=True)
                except ValueError:
                    print(f"‚ùå ADMIN_ID '{ADMIN_ID}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º")
            return

        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –ø–æ—Ä—Ü–∏—è–º–∏ –ø–æ 5
        batch_size = 5
        msg = "üì¨ *–î–∞–π–¥–∂–µ—Å—Ç –ø–æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏–∏ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ 7 –¥–Ω–µ–π)*\n\n"
        for i, art in enumerate(selected, 1):
            title = art['title']
            if art['lang'] == 'en':
                title = translate_text(title)
            source = art.get('source', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ')
            msg += f"üìå {title}\nüåê {source}\nüîó {art['url']}\n\n"

            if i % batch_size == 0 or i == len(selected):
                if ADMIN_ID:
                    try:
                        admin_id_int = int(ADMIN_ID)
                        send_message(admin_id_int, msg, parse_mode=None, disable_preview=False)
                    except ValueError:
                        print(f"‚ùå ADMIN_ID '{ADMIN_ID}' –Ω–µ —è–≤–ª—è–µ—Ç—Å—è —á–∏—Å–ª–æ–º")
                msg = ""
                if i != len(selected):
                    msg = "\n"

        # –û–±–Ω–æ–≤–ª—è–µ–º –∫–µ—à
        for art in selected:
            url = art.get('url')
            if url:
                seen_urls.add(url)
        save_cache(seen_urls)

        print("‚úÖ –†–∞—Å—Å—ã–ª–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")

    except Exception as e:
        error_msg = f"{type(e).__name__}: {str(e)[:500]}"
        print(f"üî¥ –û—à–∏–±–∫–∞: {error_msg}")
        if ADMIN_ID and TOKEN:
            send_message(ADMIN_ID, f"‚ùå –û—à–∏–±–∫–∞: `{error_msg}`", parse_mode=None)

# --- –ó–∞–ø—É—Å–∫ ---
if __name__ == "__main__":
    main()
